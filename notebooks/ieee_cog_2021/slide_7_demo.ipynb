{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6d7433",
   "metadata": {},
   "source": [
    "# Demo Time\n",
    "\n",
    "## Taking audience suggestions for reward wrapper\n",
    "\n",
    "### * Random Network Distillation (default)\n",
    "\n",
    "### * Autoencoder Loss\n",
    "\n",
    "### * Prediction Bonus\n",
    "\n",
    "## Rule Set\n",
    "\n",
    "###  * Move aka Morley B368/S245 (default, dynamic, includes a common puffer) \n",
    "\n",
    "### * Conway's Game of Life B3/S23 (classic)\n",
    "\n",
    "### * Life Without Death B3/S012345678 (Organic growth patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from carle.env import CARLE\n",
    "from carle.mcl import CornerBonus, SpeedDetector, PufferDetector, AE2D, RND2D, PredictionBonus\n",
    "from game_of_carle.agents.harli import HARLI\n",
    "from game_of_carle.agents.carla import CARLA\n",
    "from game_of_carle.agents.grnn import ConvGRNN\n",
    "from game_of_carle.agents.toggle import Toggle\n",
    "\n",
    "import bokeh\n",
    "import bokeh.io as bio\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import TextInput, Button, Paragraph\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "\n",
    "from bokeh.events import DoubleTap, Tap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "my_cmap = plt.get_cmap(\"magma\")\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ea35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (0):\n",
    "    device_string = \"cuda:0\"\n",
    "else:\n",
    "    device_string = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy_list = []\n",
    "\n",
    "directory_list = os.listdir(\"../../policies/\")\n",
    "\n",
    "for filename in directory_list:\n",
    "    \n",
    "    if \"HARLI\" in filename and \"glider\" in filename:\n",
    "        \n",
    "        policy_list.append(os.path.join(\"..\", \"..\", \"policies\", filename))\n",
    "        \n",
    "policy_list.sort()\n",
    "# instantiate CARLE with a speed detection wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45992a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose your own adventure\n",
    "obs_dim = 96\n",
    "\n",
    "env = CARLE(height=obs_dim, width=obs_dim, device=device_string)\n",
    "\n",
    "if(1):\n",
    "    env = RND2D(env)\n",
    "    env.batch_size=16\n",
    "elif(1):\n",
    "    env = AE2D(env)\n",
    "    env.batch_size=16\n",
    "elif(1):\n",
    "    env = PredictionBonus(env)\n",
    "    env.batch_size=16\n",
    "else:\n",
    "    env = SpeedDetector(env)\n",
    "    env.batch_size=16\n",
    "    \n",
    "    \n",
    "if(1):\n",
    "    env.rules_from_string(\"B368/S245\")\n",
    "elif(1):\n",
    "    env.rules_from_string(\"B3/S23\")\n",
    "else:\n",
    "    env.rules_from_string(\"B3/S012345678\")\n",
    "    \n",
    "    \n",
    "agent.set_params(np.load(policy_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce3720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def modify_doc(doc):\n",
    "        \n",
    "    \n",
    "    dim_wh = 24\n",
    "    dim_ww = 24\n",
    "    \n",
    "    global obs\n",
    "    obs = env.reset()\n",
    "    my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "    \n",
    "    p = figure(plot_width=3*256, plot_height=3*256, title=\"CA Universe\")\n",
    "    p_plot = figure(plot_width=int(1.25*256), plot_height=int(1.25*256), title=\"'Reward'\")\n",
    "    p_weights = figure(plot_width=int(1.255*256), plot_height=int(1.25*256), title=\"Weights\")\n",
    "\n",
    "    global my_period\n",
    "    global number_agents\n",
    "    global agent_number\n",
    "    global agent_on\n",
    "    global action\n",
    "    global reward_sum\n",
    "    \n",
    "    reward_sum = 0.0\n",
    "    \n",
    "    agent_number = 0\n",
    "    number_agents = len(policy_list)\n",
    "    my_period = 512\n",
    "    \n",
    "    agent_on = False\n",
    "    action = torch.zeros(1, 1, env.action_height, env.action_width)\n",
    "    \n",
    "    \n",
    "    # add a circle renderer with x and y coordinates, size, color, and alpha\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(my_image=[obs.squeeze().cpu().numpy()]))\n",
    "    source_plot = ColumnDataSource(data=dict(x=np.arange(1), y=np.arange(1)*0))\n",
    "    \n",
    "    source_weights = ColumnDataSource(data=dict(my_image=[my_weights]))\n",
    "    \n",
    "    img = p.image(image='my_image',x=0, y=0, dw=256, dh=256, palette=\"Magma256\", source=source)\n",
    "    line_plot = p_plot.line(line_width=3, color=\"firebrick\", source=source_plot)\n",
    "    \n",
    "    img_w = p_weights.image(image='my_image',x=0, y=0, dw=240, dh=240, palette=\"Magma256\", source=source_weights)\n",
    "    \n",
    "    button_go = Button(sizing_mode=\"stretch_width\", label=\"Run >\")     \n",
    "    button_slower = Button(sizing_mode=\"stretch_width\",label=\"<< Slower\")\n",
    "    button_faster = Button(sizing_mode=\"stretch_width\",label=\"Faster >>\")\n",
    "    \n",
    "    input_birth = TextInput(value=f\"{env.birth}\")\n",
    "    input_survive = TextInput(value=f\"{env.survive}\")\n",
    "\n",
    "    button_birth = Button(sizing_mode=\"stretch_width\", label=\"Update Birth Rules\")\n",
    "    button_survive = Button(sizing_mode=\"stretch_width\", label=\"Update Survive Rules\")\n",
    "    \n",
    "    button_reset_prev_agent = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ Prev. Agent\")\n",
    "    button_reset_this_agent = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ This Agent\")\n",
    "    button_reset_next_agent = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ Next Agent\")\n",
    "    \n",
    "    button_reset_w_spaceship = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ Spaceship\")\n",
    "    button_reset_w_glider = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ Glider\")\n",
    "    \n",
    "    button_agent_switch = Button(sizing_mode=\"stretch_width\", label=\"Turn Agent On\")\n",
    "    \n",
    "    message = Paragraph()\n",
    "    \n",
    "    def update():\n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global action\n",
    "        global agent_on\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global reward_sum\n",
    "               \n",
    "        obs, r, d, i = env.step(action)\n",
    "        rewards = np.append(rewards, r.cpu().detach().numpy().item())\n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        \n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        #new_line = dict(x=np.arange(my_step+2), y=rewards)\n",
    "        new_line = dict(x=[my_step], y=[r.cpu().detach().numpy().item()])\n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2000)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        my_step += 1\n",
    "        reward_sum += r.item()\n",
    "        message.text = f\"agent {agent_number}, step {my_step}, reward: {r.item():.4f}, mean reward per step: {(reward_sum/my_step):.4f} \\n\"\\\n",
    "                f\"{policy_list[agent_number]}\"\n",
    "        \n",
    "        \n",
    "    def go():\n",
    "       \n",
    "        if button_go.label == \"Run >\":\n",
    "            my_callback = doc.add_periodic_callback(update, my_period)\n",
    "            button_go.label = \"Pause\"\n",
    "            #doc.remove_periodic_callback(my_callback)\n",
    "            \n",
    "        else:\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "    \n",
    "    def faster():\n",
    "        \n",
    "        \n",
    "        global my_period\n",
    "        my_period = max([my_period * 0.5, 1])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "    def slower():\n",
    "        \n",
    "        global my_period\n",
    "        my_period = min([my_period * 2, 8192])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "    def reset_w_spaceship():\n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        global reward_sum\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        my_step = 0\n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()\n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        agent.reset()\n",
    "                \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            # add a spaceship to the action\n",
    "            action[:, :, 32, 32:34] = 1.0\n",
    "            action[:, :, 33, 29:32] = 1.0\n",
    "            action[:, :, 33, 33:35] = 1.0\n",
    "            action[:, :, 34, 29:34] = 1.0\n",
    "            action[:, :, 35, 30:33] = 1.0\n",
    "\n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "                \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"agent {agent_number}, step {my_step} \\n\"\\\n",
    "                f\"{policy_list[agent_number]}\"\n",
    "        \n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "    def reset_w_glider():\n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        global use_spaceship\n",
    "        global reward_sum\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        my_step = 0\n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()\n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        agent.reset()\n",
    "                \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            # add a glider to the action\n",
    "            action[:, :, 34, 32] = 1.0\n",
    "            action[:, :, 33, 32:34] = 1.0\n",
    "            action[:, :, 32, 31] = 1.0\n",
    "            action[:, :, 32, 33] = 1.0\n",
    "\n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "                \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "            \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"agent {agent_number}, step {my_step} \\n\"\\\n",
    "                f\"{policy_list[agent_number]}\"\n",
    "        \n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "    def reset_this_agent():\n",
    "       \n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        global use_spaceship\n",
    "        global reward_sum\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        \n",
    "        my_step = 0\n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()\n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        agent.reset()\n",
    "                \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            \n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"agent {agent_number}, step {my_step} \\n\"\\\n",
    "                f\"{policy_list[agent_number]}\"\n",
    "        \n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "    \n",
    "    def reset_next_agent():\n",
    "       \n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        global reward_sum\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        \n",
    "        my_step = 0\n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()        \n",
    "                      \n",
    "        agent_number = (agent_number + 1) % number_agents\n",
    "        \n",
    "        agent.set_params(np.load(policy_list[agent_number]))\n",
    "        agent.reset()\n",
    "        \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "\n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"agent {agent_number}, step {my_step}\\n\"\\\n",
    "                f\"{policy_list[agent_number]}\"        \n",
    "       \n",
    "        \n",
    "    def reset_prev_agent():\n",
    "       \n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        global reward_sum\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        \n",
    "        my_step = 0\n",
    "        \n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()        \n",
    "                \n",
    "        agent_number = (agent_number - 1) % number_agents\n",
    "        \n",
    "        agent.set_params(np.load(policy_list[agent_number]))\n",
    "        agent.reset()\n",
    "        \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"agent {agent_number}, step {my_step}\\n\"\\\n",
    "                f\"{policy_list[agent_number]}\"        \n",
    "       \n",
    "    \n",
    "    def set_birth_rules():\n",
    "        env.birth_rule_from_string(input_birth.value)\n",
    "\n",
    "        my_message = \"Rules updated to B\"\n",
    "\n",
    "        for elem in env.birth:\n",
    "            my_message += str(elem)\n",
    "        my_message += \"/S\"    \n",
    "\n",
    "        for elem in env.survive:\n",
    "            my_message += str(elem)\n",
    "\n",
    "        message.text = my_message\n",
    "\n",
    "    def set_survive_rules():\n",
    "        env.survive_rule_from_string(input_survive.value)\n",
    "\n",
    "        my_message = \"Rules updated to B\"\n",
    "\n",
    "        for elem in env.birth:\n",
    "            my_message += str(elem)\n",
    "        my_message += \"/S\"    \n",
    "\n",
    "        for elem in env.survive:\n",
    "            my_message += str(elem)\n",
    "\n",
    "        message.text = my_message\n",
    "   \n",
    "    def human_toggle(event):\n",
    "        global action\n",
    "        \n",
    "        coords = [np.round(env.height*event.y/256-0.5), np.round(env.width*event.x/256-0.5)]\n",
    "        offset_x = (env.height - env.action_height) / 2\n",
    "        offset_y = (env.width - env.action_width) / 2\n",
    "   \n",
    "        coords[0] = coords[0] - offset_x\n",
    "        coords[1] = coords[1] - offset_y\n",
    "        \n",
    "        coords[0] = np.uint8(np.clip(coords[0], 0, env.action_height-1))\n",
    "        coords[1] = np.uint8(np.clip(coords[1], 0, env.action_height-1))\n",
    "       \n",
    "        action[:, :, coords[0], coords[1]] = 1.0 * (not(action[:, :, coords[0], coords[1]]))\n",
    "   \n",
    "        #padded_action = stretch_pixel/2 + env.action_padding(action).squeeze()\n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "    def clear_toggles():\n",
    "        global action\n",
    "        \n",
    "        if button_go.label == \"Pause\":\n",
    "            \n",
    "            action *= 0\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "\n",
    "            padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "\n",
    "            my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            my_img[my_img > 3.0] = 3.0\n",
    "            (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            new_data = dict(my_image=[my_img])\n",
    "\n",
    "            source.stream(new_data, rollover=8)\n",
    "        else:\n",
    "            doc.add_periodic_callback(update, my_period)\n",
    "            button_go.label = \"Pause\"\n",
    "            \n",
    "    def agent_on_off():\n",
    "        global agent_on\n",
    "        \n",
    "        if button_agent_switch.label == \"Turn Agent Off\":\n",
    "            agent_on = False\n",
    "            button_agent_switch.label = \"Turn Agent On\"\n",
    "                \n",
    "        else:\n",
    "            agent_on = True\n",
    "            button_agent_switch.label = \"Turn Agent Off\"\n",
    "            \n",
    "    \n",
    "   \n",
    "    reset_w_glider()\n",
    "        \n",
    "    p.on_event(Tap, human_toggle)\n",
    "    p.on_event(DoubleTap, clear_toggles)\n",
    "    \n",
    "    button_reset_prev_agent.on_click(reset_prev_agent)\n",
    "    button_reset_this_agent.on_click(reset_this_agent)\n",
    "    button_reset_next_agent.on_click(reset_next_agent)\n",
    "\n",
    "    button_reset_w_glider.on_click(reset_w_glider)\n",
    "    button_reset_w_spaceship.on_click(reset_w_spaceship)\n",
    "    \n",
    "    button_birth.on_click(set_birth_rules)\n",
    "    button_survive.on_click(set_survive_rules)\n",
    "\n",
    "    button_go.on_click(go)\n",
    "    button_faster.on_click(faster)\n",
    "    button_slower.on_click(slower)\n",
    "    button_agent_switch.on_click(agent_on_off)\n",
    "    \n",
    "    control_layout = row(button_slower, button_go, button_faster)\n",
    "    reset_layout = row(button_reset_prev_agent, button_reset_this_agent, button_reset_next_agent)\n",
    "    spaceship_layout = row(button_reset_w_glider, button_reset_w_spaceship)\n",
    "    rule_layout = row(input_birth, button_birth, input_survive, button_survive)\n",
    "        \n",
    "    agent_toggle_layout = row(button_agent_switch)\n",
    "    \n",
    "    display_layout = row(p, column(p_plot, p_weights))\n",
    "    message_layout = row(message)\n",
    "    \n",
    "    doc.add_root(display_layout)\n",
    "    doc.add_root(control_layout)\n",
    "    doc.add_root(spaceship_layout)\n",
    "    doc.add_root(reset_layout)\n",
    "    doc.add_root(rule_layout)\n",
    "    doc.add_root(message_layout)\n",
    "    doc.add_root(agent_toggle_layout)\n",
    "    \n",
    "\n",
    "show(modify_doc)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
