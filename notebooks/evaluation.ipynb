{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553f16d9",
   "metadata": {},
   "source": [
    "# CARLE's Game Evaluation Notebook\n",
    "\n",
    "## Agent/Policy Description\n",
    "\n",
    "## Voting Instructions\n",
    "\n",
    "Voting instructions will be added here in a later update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c91b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from carle.env import CARLE\n",
    "from carle.mcl import CornerBonus, SpeedDetector, PufferDetector, AE2D, RND2D\n",
    "from game_of_carle.agents.harli import HARLI\n",
    "from game_of_carle.agents.carla import CARLA\n",
    "from game_of_carle.agents.grnn import ConvGRNN\n",
    "from game_of_carle.agents.toggle import Toggle\n",
    "\n",
    "import bokeh\n",
    "import bokeh.io as bio\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import TextInput, Button, Paragraph\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "from bokeh.events import DoubleTap, Tap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "my_cmap = plt.get_cmap(\"magma\")\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trained with the SpeedDetector and RND2D bonus wrappers with the B358/S245 Morely rules,\n",
    "some of these agents learned that they could game that reward system by exploiting a chaotic boundary with \n",
    "essentially random actions. This is something of a specification gaming/reward hacking strategy, as it is \n",
    "unlikely for agents to learn more interesting strategies when they already get a high reward from cells near \n",
    "the border transiently becoming active. One way to improve this would be to restrict agent activity to the center \n",
    "of the action space (like with the Toggle agent), which yields a buffer between what the agent modifies and\n",
    "what the speed reward wrapper uses to calculate center of mass. Likewise the reward wrapper could be modified to \n",
    "include a 'frontier zone' itself.\n",
    "\n",
    "Occasionally agents exhibit a 'wave' strategy, where toggling all the cells at the action space boundary \n",
    "creates a diminishing line of active cells that propagates toward the CA grid edges. For CARLA agents, this \n",
    "strategy mostly if not only is used immediately after resetting the environment/agent.\n",
    "\"\"\"\n",
    "\n",
    "agent = CARLA()\n",
    "\n",
    "params_list = [\\\n",
    "        \"../policies/CARLA_42_glider_rnd2d_experiment1622466808best_params_gen31.npy\",\\\n",
    "        \"../policies/CARLA_43110_glider_rnd2d_experiment1622503099best_params_gen31.npy\"] \n",
    "\n",
    "# choose parameters to load\n",
    "params_index = 0\n",
    "\n",
    "agent.set_params(np.load(params_list[params_index]))\n",
    "\n",
    "env = CARLE(height=128, width=128)\n",
    "env = SpeedDetector(env)\n",
    "\n",
    "my_rules = \"B368/S245\"\n",
    "\n",
    "env.rules_from_string(my_rules)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Toggle agent parameters become the actions at step 0, after which the agent does nothing until `agent.reset` \n",
    "is called. In other words you can use the Toggle agent to optimize an initial pattern directly. \n",
    "\n",
    "Using CMA-ES with this strategy and SpeedDetector + RND2D reward wrappers pretty reliably finds patterns than \n",
    "coalesce into a moving machine, although so far the pattern always turns into either a jellyfish glider or\n",
    "the common puffer. \n",
    "\"\"\"\n",
    "\n",
    "agent =  Toggle()\n",
    "\n",
    "#my_params = np.load(\"../policies/Toggle_13_glider_rnd2d_experiment1622420340best_params_gen31.npy\") # glider\n",
    "#my_params = np.load(\"../policies/Toggle_1337_glider_rnd2d_experiment1622437453best_params_gen31.npy\") # glider\n",
    "#my_params = np.load(\"../policies/Toggle_42_glider_rnd2d_experiment1622455453best_params_gen31.npy\") # glider\n",
    "#my_params = np.load(\"../policies/Toggle_12345_glider_rnd2d_experiment1622474002best_params_gen31.npy\") # puffer\n",
    "#my_params = np.load(\"../policies/Toggle_43110_glider_rnd2d_experiment1622491637best_params_gen31.npy\") # puffer\n",
    "\n",
    "params_list = [\"../policies/Toggle_13_glider_rnd2d_experiment1622420340best_params_gen31.npy\", \\\n",
    "        \"../policies/Toggle_1337_glider_rnd2d_experiment1622437453best_params_gen31.npy\",\\\n",
    "        \"../policies/Toggle_42_glider_rnd2d_experiment1622455453best_params_gen31.npy\",\\\n",
    "        \"../policies/Toggle_12345_glider_rnd2d_experiment1622474002best_params_gen31.npy\",\\\n",
    "        \"../policies/Toggle_43110_glider_rnd2d_experiment1622491637best_params_gen31.npy\"]\n",
    "\n",
    "params_index = 0\n",
    "\n",
    "agent.set_params(np.load(params_list[params_index]))\n",
    "\n",
    "env = CARLE(height=128, width=128)\n",
    "env = SpeedDetector(env)\n",
    "\n",
    "my_rules = \"B368/S245\"\n",
    "\n",
    "env.rules_from_string(my_rules)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trained with the SpeedDetector and RND2D bonus wrappers with the B358/S245 Morely rules,\n",
    "some of these agents learned that they could game that reward system by exploiting a chaotic boundary with \n",
    "essentially random actions. This is something of a specification gaming/reward hacking strategy, as it is \n",
    "unlikely for agents to learn more interesting strategies when they already get a high reward from cells near \n",
    "the border transiently becoming active. One way to improve this would be to restrict agent activity to the center \n",
    "of the action space (like with the Toggle agent), which yields a buffer between what the agent modifies and\n",
    "what the speed reward wrapper uses to calculate center of mass. Likewise the reward wrapper could be modified to \n",
    "include a 'frontier zone' itself.\n",
    "\n",
    "There is an interesting reward hack that is sometimes exhibited by HARLI agents trained with SpeedDetector. \n",
    "CARLE instances can be reset if the agent toggles every cell in the action space simultaneously, and this can generate\n",
    "high rewards in one of two ways. The first is that if there are many live cells outside the action space these will\n",
    "be zeroed out when the environment is reset, making for an large change in the center of mass of all live cells as\n",
    "is used by SpeedDetector to calculate rewards. The second is that this sets up the environment perfectly for a \n",
    "highly rewarding \"wave\" strategy, where a line of active cells at the action space boundary sets up conditions for \n",
    "a fast moving line of live cells to propagate toward the grid edge. When both those mechanisms are combined, \n",
    "the rewards can be quite high, although we probably would have preferred agents that come up with or rediscover\n",
    "interesting glider and spaceship patterns. \n",
    "\"\"\"\n",
    "\n",
    "agent = HARLI()\n",
    "\n",
    "params_list = [\"../policies/HARLI_13_glider_rnd2d_experiment1622423202best_params_gen31.npy\", \\\n",
    "        \"../policies/HARLI_42_glider_rnd2d_experiment1622458272best_params_gen31.npy\",\\\n",
    "        \"../policies/HARLI_1337_glider_rnd2d_experiment1622440720best_params_gen31.npy\",\\\n",
    "        \"../policies/HARLI_12345_glider_rnd2d_experiment1622477110best_params_gen31.npy\",\\\n",
    "        \"../policies/HARLI_43110_glider_rnd2d_experiment1622494528best_params_gen31.npy\"]\n",
    "\n",
    "# choose parameters to load\n",
    "params_index = 0\n",
    "\n",
    "agent.set_params(np.load(params_list[params_index]))\n",
    "\n",
    "env = CARLE(height=128, width=128)\n",
    "env = SpeedDetector(env)\n",
    "#env = AE2D(env)\n",
    "\n",
    "my_rules = \"B368/S245\"\n",
    "\n",
    "env.rules_from_string(my_rules)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82c549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def modify_doc(doc):\n",
    "        \n",
    "    #agent = SubmissionAgent()\n",
    "    #agent.toggle_rate = 0.48\n",
    "    global obs\n",
    "    obs = env.reset()\n",
    "    \n",
    "    p = figure(plot_width=3*256, plot_height=3*256, title=\"CA Universe\")\n",
    "    p_plot = figure(plot_width=int(2.5*256), plot_height=int(2.5*256), title=\"'Reward'\")\n",
    "\n",
    "    global my_period\n",
    "    global number_agents\n",
    "    global agent_number\n",
    "    \n",
    "    agent_number = 0\n",
    "    number_agents = len(params_list)\n",
    "    my_period = 512    \n",
    "    \n",
    "    source = ColumnDataSource(data=dict(my_image=[obs.squeeze().cpu().numpy()]))\n",
    "    source_plot = ColumnDataSource(data=dict(x=np.arange(1), y=np.arange(1)*0))\n",
    "    \n",
    "    img = p.image(image='my_image',x=0, y=0, dw=256, dh=256, palette=\"Magma256\", source=source)\n",
    "    line_plot = p_plot.line(line_width=3, color=\"firebrick\", source=source_plot)\n",
    "    \n",
    "    button_go = Button(sizing_mode=\"stretch_width\", label=\"Run >\")     \n",
    "    button_slower = Button(sizing_mode=\"stretch_width\",label=\"<< Slower\")\n",
    "    button_faster = Button(sizing_mode=\"stretch_width\",label=\"Faster >>\")\n",
    "    button_reset = Button(sizing_mode=\"stretch_width\",label=\"Reset\")\n",
    "    \n",
    "    button_reset_prev_agent = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ Prev. Agent\")\n",
    "    button_reset_this_agent = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ This Agent\")\n",
    "    button_reset_next_agent = Button(sizing_mode=\"stretch_width\",label=\"Reset w/ Next Agent\")\n",
    "  \n",
    "    \n",
    "    input_birth = TextInput(value=\"B\")\n",
    "    input_survive = TextInput(value=\"S\")\n",
    "    button_birth = Button(sizing_mode=\"stretch_width\", label=\"Update Birth Rules\")\n",
    "    button_survive = Button(sizing_mode=\"stretch_width\", label=\"Update Survive Rules\")\n",
    "    button_agent_switch = Button(sizing_mode=\"stretch_width\", label=\"Turn Agent Off\")\n",
    "    \n",
    "    message = Paragraph()\n",
    "    \n",
    "    def update():\n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global action\n",
    "        global agent_on\n",
    "        global my_step\n",
    "        global rewards\n",
    "        \n",
    "        obs, r, d, i = env.step(action)\n",
    "        rewards = np.append(rewards, r.cpu().numpy().item())\n",
    "        if agent_on:\n",
    "            action = agent(obs) #1.0 * (torch.rand(env.instances,1,env.action_height,env.action_width) < 0.05)\n",
    "        else:\n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        #padded_action = stretch_pixel/2 + env.action_padding(action).squeeze()\n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        #new_line = dict(x=np.arange(my_step+2), y=rewards)\n",
    "        new_line = dict(x=[my_step], y=[r.cpu().numpy().item()])\n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2000)\n",
    "        \n",
    "        my_step += 1\n",
    "        message.text = f\"agent {agent_number}, step {my_step}, reward: {r.item()} \\n\"\\\n",
    "                f\"{params_list[agent_number]}\"\n",
    "        \n",
    "        \n",
    "    def go():\n",
    "       \n",
    "        if button_go.label == \"Run >\":\n",
    "            my_callback = doc.add_periodic_callback(update, my_period)\n",
    "            button_go.label = \"Pause\"\n",
    "            #doc.remove_periodic_callback(my_callback)\n",
    "            \n",
    "        else:\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "    \n",
    "    def faster():\n",
    "        \n",
    "        \n",
    "        global my_period\n",
    "        my_period = max([my_period * 0.5, 1])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "    def slower():\n",
    "        \n",
    "        global my_period\n",
    "        my_period = min([my_period * 2, 8192])\n",
    "        go()\n",
    "        go()\n",
    "    \n",
    "    def reset():\n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        \n",
    "        my_step = 0\n",
    "        \n",
    "        obs = env.reset()        \n",
    "        agent.reset()\n",
    "                \n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        new_data = dict(my_image=[(stretch_pixel + obs.squeeze()).cpu().numpy()])\n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "    \n",
    "    def reset_this_agent():\n",
    "       \n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        \n",
    "        my_step = 0\n",
    "        \n",
    "        obs = env.reset()        \n",
    "        agent.reset()\n",
    "                \n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        new_data = dict(my_image=[(stretch_pixel + obs.squeeze()).cpu().numpy()])\n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "    \n",
    "    def reset_next_agent():\n",
    "       \n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        \n",
    "        my_step = 0\n",
    "        \n",
    "        obs = env.reset()        \n",
    "                \n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        new_data = dict(my_image=[(stretch_pixel + obs.squeeze()).cpu().numpy()])\n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "        agent_number = (agent_number + 1) % number_agents\n",
    "        \n",
    "        agent.set_params(np.load(params_list[agent_number]))\n",
    "        agent.reset()\n",
    "        \n",
    "        message.text = f\"reset with agent {agent_number}\"\n",
    "        \n",
    "    def reset_prev_agent():\n",
    "       \n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global agent_number\n",
    "        global number_agents\n",
    "        \n",
    "        my_step = 0\n",
    "        \n",
    "        obs = env.reset()        \n",
    "                \n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        new_data = dict(my_image=[(stretch_pixel + obs.squeeze()).cpu().numpy()])\n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "        agent_number = (agent_number - 1) % number_agents\n",
    "        \n",
    "        agent.set_params(np.load(params_list[agent_number]))\n",
    "        agent.reset()\n",
    "        \n",
    "        message.text = f\"reset with agent {agent_number}\"\n",
    "    \n",
    "    \n",
    "    def set_birth_rules():\n",
    "        env.birth_rule_from_string(input_birth.value)\n",
    "        \n",
    "        my_message = \"Rules updated to B\"\n",
    "        \n",
    "        for elem in env.birth:\n",
    "            my_message += str(elem)\n",
    "        my_message += \"/S\"    \n",
    "        \n",
    "        for elem in env.survive:\n",
    "            my_message += str(elem)\n",
    "            \n",
    "        message.text = my_message\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    def set_survive_rules():\n",
    "        env.survive_rule_from_string(input_survive.value)\n",
    "        \n",
    "        my_message = \"Rules updated to B\"\n",
    "        \n",
    "        for elem in env.birth:\n",
    "            my_message += str(elem)\n",
    "        my_message += \"/S\"    \n",
    "        \n",
    "        for elem in env.survive:\n",
    "            my_message += str(elem)\n",
    "            \n",
    "        message.text = my_message\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    def human_toggle(event):\n",
    "        global action\n",
    "        \n",
    "        coords = [np.round(env.height*event.y/256-0.5), np.round(env.width*event.x/256-0.5)]\n",
    "        offset_x = (env.height - env.action_height) / 2\n",
    "        offset_y = (env.width - env.action_width) / 2\n",
    "   \n",
    "        coords[0] = coords[0] - offset_x\n",
    "        coords[1] = coords[1] - offset_y\n",
    "        \n",
    "        coords[0] = np.uint8(np.clip(coords[0], 0, env.action_height-1))\n",
    "        coords[1] = np.uint8(np.clip(coords[1], 0, env.action_height-1))\n",
    "       \n",
    "        action[:, :, coords[0], coords[1]] = 1.0 * (not(action[:, :, coords[0], coords[1]]))\n",
    "   \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "    def clear_toggles():\n",
    "        global action\n",
    "        \n",
    "        if button_go.label == \"Pause\":\n",
    "            \n",
    "            action *= 0\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "\n",
    "            padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "\n",
    "            my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            my_img[my_img > 3.0] = 3.0\n",
    "            (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            new_data = dict(my_image=[my_img])\n",
    "\n",
    "            source.stream(new_data, rollover=8)\n",
    "        else:\n",
    "            doc.add_periodic_callback(update, my_period)\n",
    "            button_go.label = \"Pause\"\n",
    "            \n",
    "    def agent_on_off():\n",
    "        global agent_on\n",
    "        \n",
    "        if button_agent_switch.label == \"Turn Agent Off\":\n",
    "            agent_on = False\n",
    "            button_agent_switch.label = \"Turn Agent On\"\n",
    "                \n",
    "        else:\n",
    "            agent_on = True\n",
    "            button_agent_switch.label = \"Turn Agent Off\"\n",
    "            \n",
    "    \n",
    "    global agent_on\n",
    "    agent_on = True\n",
    "    global action\n",
    "    action = torch.zeros(1, 1, env.action_height, env.action_width)\n",
    "    \n",
    "    reset()\n",
    "    \n",
    "    \n",
    "    p.on_event(Tap, human_toggle)\n",
    "    p.on_event(DoubleTap, clear_toggles)\n",
    "    \n",
    "    \n",
    "    button_reset_prev_agent.on_click(reset_prev_agent)\n",
    "    button_reset_this_agent.on_click(reset_this_agent)\n",
    "    button_reset_next_agent.on_click(reset_next_agent)\n",
    "    \n",
    "    button_birth.on_click(set_birth_rules)\n",
    "    button_survive.on_click(set_survive_rules)\n",
    "    button_go.on_click(go)\n",
    "    button_faster.on_click(faster)\n",
    "    button_slower.on_click(slower)\n",
    "    button_reset.on_click(reset)\n",
    "    button_agent_switch.on_click(agent_on_off)\n",
    "    \n",
    "    \n",
    "    control_layout = row(button_slower, button_go, button_faster, button_reset)\n",
    "    policy_change_layout = row(button_reset_prev_agent, button_reset_this_agent, button_reset_next_agent)\n",
    "    rule_layout = row(input_birth, button_birth, input_survive, button_survive)\n",
    "    agent_toggle_layout = row(button_agent_switch)\n",
    "    \n",
    "    display_layout = row(p, p_plot)\n",
    "    message_layout = row(message)\n",
    "    \n",
    "    doc.add_root(display_layout)\n",
    "    doc.add_root(control_layout)\n",
    "    doc.add_root(policy_change_layout)\n",
    "    doc.add_root(rule_layout)\n",
    "    doc.add_root(message_layout)\n",
    "    doc.add_root(agent_toggle_layout)\n",
    "    \n",
    "\n",
    "show(modify_doc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930f907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
